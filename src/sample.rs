//! The central module of the library
//!
//! The [Sample] trait is the central piece of Monty-Carlos. By implementing it, the user defines
//! how what exactly the Monte-Carlo simulation does at each step. The basic steps of a Monte-Carlo
//! simulation essentially are:
//! 1. Randomly generate something,
//! 2. Calculate a value from the generated dataset,
//! 3. Repeat.
//!
//! The [Sample] trait covers the first two steps.

pub mod fitting;

use rand::distributions::Distribution;
use statrs::distribution::ContinuousCDF;

use fitting::FittingDistribution;

/// A trait describing how to generate and evaluate samples for a Monte-Carlo simulation.
///
/// The type implementing this trait should contain all the necessary probability distributions,
/// as well as a way to store the generated dataset.
pub trait Sample {
    /// This method randomly generates a dataset using `rng` and store it.
    fn generate(&mut self, rng: &mut impl rand::Rng);

    /// This method calculates a statistic from the stored dataset and returns it.
    ///
    /// The implementors are allowed to assume that the method [`Sample::generate`] was called at least once
    /// before this one. The statistic must be calculated for the dataset generated by the most
    /// recent call of [`Sample::generate`].
    fn evaluate(&self) -> f64;
}

/// An implementor of [Sample] for simulating the Kolmogorov-Smirnov test.
///
/// [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test) evaluates how likely is it, for the empirical dataset to come from a
/// theoretically predicted (i.e. apriori) distribution.
///
/// Given an apriori CDF `F` and an empirical dataset `X` with `n` datapoints, the
/// Kolmogorov-Smirnov test first direct one to construct the empirical distribution function
/// `F_n`:
/// ```math
/// F_n(x) = #{i | X_i <= x}/n,
/// ```
/// where `#A` denotes the number of elements of a set `A`.
///
/// Next, one calculates the maximal deviation `D_max` of the empirical distribution `F_n` from the
/// theritical distribution `F`:
/// ```math
/// D_max = sup_x {|F_n (x) - F(x)|}
/// ```
///
/// [`KSSample`] is designed for the simulation of the exactly such process. It's configured with the
/// theoretical distribution with the CDF `F` and the number of datapoints of simulated datasets
/// `n`.
#[allow(clippy::module_name_repetitions)]
pub struct KSSample<D> {
    distr: D,
    // Storage for the generated dataset.
    // Avoids large allocations during the simulation.
    samples: Box<[f64]>,
}

impl<D> Sample for KSSample<D>
where
    D: Distribution<f64>,
    D: ContinuousCDF<f64, f64>,
{
    /// Generates `self.datapoint_count()` floating-point numbers
    /// using `self.distr()` implementaion of [`Distribution<f64>`] and stores them.
    fn generate(&mut self, rng: &mut impl rand::Rng) {
        // Generates a random sorted slice of `self.datapoint_count()` floating-point numbers.
        self.samples.fill_with(|| self.distr.sample(rng));
        self.samples.sort_by(f64::total_cmp);
    }

    /// Evaluates `D_max` statistic from the stored dataset using `self.distr()` implementation of [`ContinuousCDF<f64, f64>`].
    fn evaluate(&self) -> f64 {
        self.dmax(&self.distr)
    }
}

impl<D> KSSample<D>
where
    D: ContinuousCDF<f64, f64>,
{
    /// Calculates the `D_max` statistic defined by Kolmogorov-Smirnov test between the dataset
    /// stored in `self` and the distribution `distr`.
    ///
    /// Uses `distr`'s implementation of [`ContinuousCDF`].
    ///
    /// See documentation of [`KSSample`] for the definition of `D_max`
    fn dmax(&self, distr: &D) -> f64 {
        let n = self.datapoint_count();
        // Creates an iterator of the maximal absolute deviations between each step of the
        // empirical distribution and the corresponding fragment of `distr`'s CDF.
        #[allow(clippy::cast_precision_loss)]
        let deviations = self.samples.windows(2).enumerate().map(|(i, xs)| {
            let left = f64::abs(distr.cdf(xs[0]) - (i + 1) as f64 / n as f64);
            let right = f64::abs(distr.cdf(xs[1]) - (i + 1) as f64 / n as f64);
            f64::max(left, right)
        });
        // Maximal deviation on the interval [X_0, X_{n-1})
        let middle_max = deviations.max_by(f64::total_cmp).unwrap();
        // Maximal deviation on the interval (-∞, X_0)
        let left = distr.cdf(*self.samples.first().unwrap());
        // Maximal deviation on the interval [X_{n-1}, ∞)
        let right = 1.0 - distr.cdf(*self.samples.last().unwrap());
        left.max(middle_max.max(right))
    }
}

impl<D> KSSample<D> {
    /// Constructs a [`KSSample`] instance for a Kolmogorov-Smirnov test with
    /// theoretical distribution `distr` and `count_datapoints` datapoints in the simulated
    /// datasets.
    ///
    /// Returns [None] when `datapoint_count == 0`.
    pub fn new(distr: D, datapoint_count: usize) -> Option<Self> {
        if datapoint_count == 0 {
            return None;
        }
        let samples = std::iter::repeat(0.0).take(datapoint_count).collect();
        Some(Self { distr, samples })
    }

    /// Returns the distribution used for generating datasets and evaluating the `D_max` statistic.
    pub fn distr(&self) -> &D {
        &self.distr
    }

    /// Returns the number of datapoints in the generated datasets.
    pub fn datapoint_count(&self) -> usize {
        self.samples.len()
    }
}

/// An implementor of [Sample] for simulating the Lilliefors test.
///
/// A drawback of Kolmogorov-Smirnov test is the requirement to possess a hypothesis that allows
/// one to choose a distribution apriori. When the distribution is estimated from the empirical
/// dataset, the pvalues given by the Kolmogorov-Smirnov test are no longer accurate. Hubert
/// Lilliefors computed the corrected pvalues for the normal distribution in 1969 through computer
/// simulations. [`LillieforsSample`] type is intended to allow the user to possibly replicate these
/// simulation on a contemporary computer.
///
/// Unlike the Kolmogorov-Smirnov test where the `D_max` statistic is calculated between the dataset
/// and the apriori distribution, Lilliefors test requires one to calculate the `D_max` statistic
/// between the dataset and a distribution that have been fit to it. Hence is the dependence on the
/// [`FittingDistribution`] trait.
///
/// [`LillieforsSample`] acts as a wrapper for [`KSSample`] with redefined [`Sample::evaluate`] method.
#[allow(clippy::module_name_repetitions)]
pub struct LillieforsSample<D> {
    inner: KSSample<D>,
}

impl<D> LillieforsSample<D> {
    /// Creates an instance of [`LillieforsSample`] that corresponds to the instance of
    /// [`KSSample`] created by the instances of [`KSSample::new`] with the same arguments.
    ///
    /// Returns [None] when `datapoint_count == 0`.
    pub fn new(distr: D, num_samples: usize) -> Option<Self> {
        Some(Self {
            inner: KSSample::new(distr, num_samples)?,
        })
    }

    /// Returns the distribution used by [`Self::generate`] to generate a dataset.
    pub fn distr(&self) -> &D {
        self.inner.distr()
    }
}

impl<D> Sample for LillieforsSample<D>
where
    D: Distribution<f64>,
    D: ContinuousCDF<f64, f64>,
    D: FittingDistribution,
{
    /// Behaves identically to [`KSSample::generate`].
    fn generate(&mut self, rng: &mut impl rand::Rng) {
        self.inner.generate(rng);
    }

    /// Behaves identically to [`KSSample::evaluate`], except uses the distribution fit to
    /// the stored dataset by [`FittingDistribution::fit`] instead of `self.distr()`.
    fn evaluate(&self) -> f64 {
        let sample_distr = D::fit(&self.inner.samples);
        self.inner.dmax(&sample_distr)
    }
}
